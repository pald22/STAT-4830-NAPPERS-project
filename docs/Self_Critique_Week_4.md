Self-Critique (OODA Framework)

**Observe.**


 Rereading our current report draft and browsing our repository alongside the base template and last year’s PRISM project, we see that our technical pipeline is starting to take shape but our identity as a project is still fuzzy. Our repo structure mirrors the STAT-4830 base (README, report.md, notebooks, src, docs) and is currently dominated by template language from the course repository rather than a description of our specific approach. In contrast, the PRISM repo’s README already foregrounds a clear, named method (“a dynamic portfolio optimizer using online gradient descent”) and spells out its multi-objective tradeoffs (Sortino, max drawdown, turnover, concentration via ENP) and demo. When we run our code and skim our notebooks, the computations work, but someone landing on our repo today would not immediately understand how our project is different from PRISM, despite our recent design discussions about skewness-based optimization and potential LLM-driven news inputs.

 
**Orient.**


 Our main strengths are that we have adopted a solid, modular starting point from the base project—separating data, notebooks, src, and docs—and that we already have a working baseline trading pipeline that we can extend. The structure clearly could support new objectives and data sources, and the fact that PRISM implements its optimizer in a single, well-scoped module (OnlinePortfolioOptimizer.py) suggests that our own design could similarly encapsulate our loss and update rules. However, several weaknesses stand out. First, our current written materials (README and early report text) still read like the generic course template rather than a project that explicitly differentiates itself from PRISM’s OGD-based, transaction-cost-aware optimizer; we are implicitly “doing something similar” instead of explicitly stating what is new. Second, although in our meeting we committed to an alternative optimization direction—using a skewness tensor in the loss to jointly minimize covariance and maximize portfolio skew—we have not yet reflected that decision in the code or in a precise mathematical formulation in the report. Third, our ideas around incorporating an LLM newsfeed (e.g., using a BloombergGPT-style model as a source of summarized news or sentiment signals) are still at the level of brainstorming: there is no concrete definition of how such a signal would enter the feature set or loss, nor any placeholder experiment or evaluation metric that would highlight its effect. Compared to PRISM’s clearly articulated metrics and demo, we currently under-communicate both our method and our intended contribution.

 
**Decide.**


 For the next draft, the most important decision is to explicitly reframe the project around our chosen differentiators and then align the code and report to that framing. Concretely, we will rewrite the problem statement and methods sections to say that our core objective is to construct portfolios that do not just control variance and transaction costs, but that explicitly optimize higher-order moments by using a skewness tensor to encourage positively skewed return distributions while penalizing covariance. This should be stated as a formal loss function, side-by-side with a short comparison to PRISM’s multi-objective OGD setup, so that the novelty is obvious. In parallel, we will work with the team to implement a first version of this skewness-aware loss inside our optimizer module in src/, even if the initial implementation is simplified. As a secondary but still concrete step, we will specify one narrow, realistic way to prototype a news-based signal inspired by BloombergGPT—for example, a daily sentiment or “risk flag” score at the asset or sector level—and add at least a placeholder feature pipeline in our notebooks, so that the report can discuss this component as more than an abstract idea. Finally, we will ensure that our README and report mirror this reframed identity, in the way that PRISM’s README already does for its own method, so readers understand what our algorithm is trying to achieve before they look at any code or figures.


**Act.**


 To carry out these decisions, we will need a few specific resources and supports. On the technical side, we need to review references or worked examples on portfolio optimization with skewness tensors and higher-order moments, so that our loss function is mathematically coherent and numerically stable rather than ad hoc. We
 also need to explore accessible proxies for a BloombergGPT-like newsfeed—either public news sentiment datasets or example outputs from large financial language models—so that we can define a tractable, well-scoped way to integrate a news factor into our pipeline without relying on proprietary data we cannot access. Finally, we will benefit from a quick check-in with the course staff (and a careful reread of the base project documentation on development logs and critiques) to confirm that this skewness-centered, news-aware direction is distinct enough from PRISM and feasible within the course timeline, and to make sure we are using our repo (including docs/development_log.md) to clearly document these design choices for future drafts. 
